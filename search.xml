<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一种基于机器学习模型DeepAR的量化策略</title>
      <link href="/2023/07/21/deepar/"/>
      <url>/2023/07/21/deepar/</url>
      
        <content type="html"><![CDATA[<p>目前国内百亿规模的量化私募基金已经超过 100 只, 并且很多都在积极的从传统的量化策略转向机器学习模型预测生成的策略。<br>本文介绍基于机器学习算法DeepAR， 在亚马逊的机器学习平台Sagemaker上进行训练生成的策略及其历史回测效果。</p><h2 id="时间序列数据预测模型-–-DeepAR"><a href="#时间序列数据预测模型-–-DeepAR" class="headerlink" title="时间序列数据预测模型 – DeepAR:"></a>时间序列数据预测模型 – DeepAR:</h2><p>亚马逊提供了一种使用循环神经网络（RNN）预测时间序列数据的深度学习算法。传统的预测方法，如自回归综合移动平均（ARIMA）或指数平滑（ETS）算法将单个模型拟合到每个单独的时间序列。然后使用该模型对此时间序列进行预测。<br>然而，在许多应用程序中有许多类似的时间序列。例如，我们可能有时间序列分组: 针对多个不同产品的需求, 多个服务器负载, 多个网页访问量。对于这种类型的应用程序，可以在所有时间序列上联合训练单个模型。DeepAR采用了这种方法,当数据集包含数百个相关的时间序列时，DeepAR的性能优于标准ARIMA和ETS方法。我们还可以使用经过训练的模型为类似的新的时间序列生成预测。<br>本文不会详细介绍DeepAR算法，请参考论文：</p><p><code>https://arxiv.org/pdf/1704.04110.pdf</code></p><p>GluonTS 也是一个比较常用的时间序列预测模型：<br><code>https://ts.gluon.ai/stable/getting_started/background.html</code></p><p>我会用相同的数据去训练GluonTS模型 ，和DeepAR进行对比测试。</p><h2 id="训练数据预处理"><a href="#训练数据预处理" class="headerlink" title="训练数据预处理"></a>训练数据预处理</h2><ul><li><p>金融数据获取<br>  我们可以通过多种途径获取金融数据，业内的许多公司会购买Wind、恒生聚源等数据提供商的数据库，若尚未入行，则也可以通过非常多的第三方策略平台获取免费数据，例如优矿、聚宽、米筐等。</p><p>  <code> https://tushare.pro/</code></p><p>  Tushare是一个免费、开源的python财经数据接口包，不带任何商业性质和目的。数据内容包含股票、基金、期货、债券、外汇、行业大数据等，同时包括了数字货币行情等区块链数据的全数据品类的金融大数据平台，为各类金融投资和研究人员提供适用的数据和工具。</p></li><li><p>数据预处理: 为了让模型更具有普适性, 需要对训练数据进行处理: 标准化处理, 中性化处理, 缺失值处理, 归一化等</p></li></ul><p>未完待续。。。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量化交易 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>均方根误差 RMSE(root mean square error)</title>
      <link href="/2023/06/28/rmse/"/>
      <url>/2023/06/28/rmse/</url>
      
        <content type="html"><![CDATA[<h2 id="MSE-mean-square-error-均方差"><a href="#MSE-mean-square-error-均方差" class="headerlink" title="MSE (mean square error) 均方差"></a>MSE (mean square error) 均方差</h2><p>真实值和预测值的方差求平均:</p><img src="/2023/06/28/rmse/RMSE-1.png" class=""><h2 id="RMSE-root-mean-square-error-均方根误差"><a href="#RMSE-root-mean-square-error-均方根误差" class="headerlink" title="RMSE (root mean square error) 均方根误差"></a>RMSE (root mean square error) 均方根误差</h2><p>MSE的平方根</p><img src="/2023/06/28/rmse/RMSE-2.png" class=""><p>RMSE 与标准差计算公式类似, 区别如下:<br>标准差是用来衡量一组数据自身的离散程度, 数据的每个元素与平均值的偏差.<br>RMSE 用来衡量预测值和真实值的偏差.</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数 - Activation Function</title>
      <link href="/2023/06/20/jhhs/"/>
      <url>/2023/06/20/jhhs/</url>
      
        <content type="html"><![CDATA[<p>激活函数就是在神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。对于神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。引入激活函数是为了增加神经网络模型的非线性。没有激活函数的每层都相当于矩阵相乘,是上一层的线性函数。就算你叠加了若干层之后，无非还是个矩阵相乘罢了,是最原始的感知机。</p><img src="/2023/06/20/jhhs/jihuohanshu-1.webp" class="" title="图1"><p>以下是一些常用的激活函数及其特性。选择哪种激活函数取决于你正在解决的具体问题和你正在使用的神经网络架构。</p><h2 id="1-Sigmoid函数"><a href="#1-Sigmoid函数" class="headerlink" title="1. Sigmoid函数"></a>1. Sigmoid函数</h2><p>Sigmoid 函数是一种常用的激活函数，主要用于二分类问题。它可以将任何值都映射到一个位于(0,1)之间的值， 常被用作神经网络的阈值函数。因此，它非常适合表示概率。Sigmoid 函数的公式为：</p><p>$$f(x) &#x3D; \frac{1}{1 + e^{-x}}$$</p><p>然而，Sigmoid 函数在输入值非常大或非常小的时候会出现梯度消失的问题，这会导致在这些区域内神经网络的学习速度变得非常慢。</p><p>函数图像如下：</p><img src="/2023/06/20/jhhs/jihuohanshu-3.jpg" class=""><h2 id="2-Tanh函数"><a href="#2-Tanh函数" class="headerlink" title="2. Tanh函数"></a>2. Tanh函数</h2><p>Tanh（双曲正切）函数是由基本双曲函数双曲正弦和双曲余弦推导而来， 与Sigmoid函数类似，但它将输入值映射到-1到1之间，而不是0到1之间。这意味着Tanh函数的输出是零中心化的，这在某些情况下可能是一个优点。Tanh函数的公式为：</p><p>$$f(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}}$$</p><p>和Sigmoid函数一样，Tanh函数在输入值非常大或非常小的时候也会出现梯度消失的问题。</p><p>函数图像如下</p><img src="/2023/06/20/jhhs/jihuohanshu-5.jpg" class=""><h2 id="3-ReLU函数"><a href="#3-ReLU函数" class="headerlink" title="3. ReLU函数"></a>3. ReLU函数</h2><p>ReLU（Rectified Linear Unit）函数是最常用的激活函数之一， 用于隐层神经元输出。当输入值小于0时，ReLU函数的输出为0；当输入值大于0时，ReLU函数的输出等于输入值。ReLU函数的公式为：</p><p>$$f(x) &#x3D; max(0, x)$$</p><p>ReLU函数在处理正数时不会出现梯度消失的问题，因此在实践中常常被优先选择。</p><p>函数图像如下:</p><img src="/2023/06/20/jhhs/jihuohanshu-7.jpg" class=""><h2 id="4-Softmax-函数"><a href="#4-Softmax-函数" class="headerlink" title="4. Softmax 函数"></a>4. Softmax 函数</h2><p>Softmax函数通常用于多分类问题中。它可以将一组输入值转化为一组表示概率分布的输出值。Softmax函数的公式为：</p><p>$$f(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j}e^{x_j}}$$</p><p>其中$x_i$表示输入向量中的第i个元素。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CNN 卷积神经网络概述</title>
      <link href="/2023/06/18/cnn/"/>
      <url>/2023/06/18/cnn/</url>
      
        <content type="html"><![CDATA[<p>卷积神经网络（CNN）是一种深度学习的网络结构，在图像识别、自然语言处理、语音识别等领域取得了巨大的成功。其基本思想是通过卷积（Convolution）操作提取输入数据的特征，利用不同层次的卷积和池化操作来逐步提取和丰富特征，最终实现分类或识别任务。</p><h2 id="结构和原理"><a href="#结构和原理" class="headerlink" title="结构和原理"></a>结构和原理</h2><p>CNN的基本结构由输入层、卷积层（convolutional layer）、池化层（pooling layer，也称为取样层）、全连接层及输出层构成。卷积层和池化层一般会取若干个，采用卷积层和池化层交替设置，即一个卷积层连接一个池化层，池化层后再连接一个卷积层，依此类推。</p><p>输入层：输入层负责接收原始数据，可以是图像、文本、声音等不同类型的数据。对于图像处理任务，输入层通常是将图像像素的灰度或彩色值作为输入数据。</p><p>卷积层：卷积层是CNN的核心，通过卷积操作对输入数据进行特征提取。卷积操作使用一个可学习的滤波器（也称为卷积核）与输入数据进行逐点乘积累加，类似于一个滑动窗口，从而在输入数据中提取出有用的特征。这个卷积核是共享参数的，在整个网络的训练过程中，包含权值的卷积核也会随之更新，直到训练完成。 </p><p>池化层：池化层用于降低数据的维度，减少计算复杂度，同时保留重要特征，增强网络的泛化能力。池化操作可以是最大池化（Max Pooling）、平均池化（Average Pooling）等。</p><p>全连接层：全连接层通常用于神经网络的输出阶段，负责输出网络的预测结果。全连接层中的每个神经元都与前一层的所有神经元相连，根据对应的连接权值进行加权求和再加上偏置值，得到该神经元输入值。 </p><p>输出层：输出层负责输出网络的预测结果。对于分类任务，输出层通常采用Softmax函数将神经元的输出值转换为概率值，从而进行分类预测；对于回归任务，输出层可以采用线性回归模型或其他回归模型进行预测。</p><p>以上是卷积神经网络的基本结构，不同的任务和数据类型可能需要进一步调整和优化网络结构以及参数设置。在实际应用中，可以使用各种优化算法（如随机梯度下降、Adam等）对网络进行训练和调优，以获得更好的性能和准确率。</p><h2 id="卷积神经网络的特点"><a href="#卷积神经网络的特点" class="headerlink" title="卷积神经网络的特点"></a>卷积神经网络的特点</h2><p>卷积神经网络由多层感知机（MLP）演变而来，由于其具有局部区域连接、权值共享、降采样的结构特点，使得卷积神经网络在图像处理领域表现出色。卷积神经网络相比于其他神经网络的特殊性主要在于权值共享与局部连接两个方面。权值共享使得卷积神经网络的网络结构更加类似于生物神经网络。局部连接不像传统神经网络那样，第n-1层的每一神经元都与第n层的所有神经元连接，而是第n-1层的神经元与第n层的部分神经元之间连接。这两个特点的作用在于降低了网络模型的复杂度，减少了权值的数目。</p><ul><li><p>局部区域连接</p><p>  在传统的神经网络结构中，神经元之间的连接是全连接的，即n-1层的神经元与n层的所有神经元全部连接。但是在卷积神经网络中，n-1层与n 层的部分神经元连接。下图展示了全连接与局部连接的区别之处，左图为全连接示意图，由图可以看出前一层到后一层神经元之间都有边存在，每条边都有参数，由此可见全连接的参数很多。右边为局部连接，由图中可以看出仅存在少量的边，可见参数减少了很多。对比左右两图可以明显看出连接数成倍的减少，相应的参数也会减少。<br>      <img src="/2023/06/18/cnn/cnn_1.png" class=""></p></li><li><p>权值共享</p><ul><li><p>什么是权值共享呢？</p><p>  其实权值共享就是整张图片在使用同一个卷积核内的参数。比如一个3<em>3</em>1的卷积核，这个卷积核内9个的参数被整张图片共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再通俗一点，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片。当然，CNN中每一个卷积层不会只有一个卷积核的，这样说只是为了方便解释。</p></li><li><p>权值共享的优点是什么呢？</p><p>  一是，权值共享的卷积操作保证了每一个像素都有一个权系数，只是这些系数被整个图片共享，因此大大减少了卷积核中参数量，降低了网络的复杂度。二是，传统的神经网络和机器学习方法需要对图像进行复杂的预处理提取特征，将得到特征再输入到神经网络中。而加入卷积操作就可以利用图片空间上的局部相关性，自动的提取特征。</p></li><li><p>那为什么卷积层会有多个卷积核呢？</p><p>  因为权值共享意味着每一个卷积核只能提取到一种特征，为了增加CNN的表达能力，需要设置多个卷积核。但是，每个卷积层中卷积核的个数是一个超参数。</p></li></ul></li><li><p>降采样</p><p>  降采样是卷积神经网络的另一重要概念，通常也称之为池化（Pooling）。最常见的方式有最大值（Max）池化、最小值（Min）池化、平均值（Average）池化。池化的好处是降低了图像的分辨率，整个网络也不容易过拟合。最大值池化如下图所示：</p>  <img src="/2023/06/18/cnn/cnn_2.png" class=""><p>  在图2是最大池化过程中，输入图像大小为4✖️4，在每2✖️2的区域中计算最大值。例如：</p>  <img src="/2023/06/18/cnn/cnn_3.png" class=""><p>  由于步长为2，因此每2✖️2的区域互不重叠，最后输出的池化特征大小为2✖️2，这个过程中分辨率变为原来的一半。</p></li></ul><h2 id="卷积神经网络的优缺点"><a href="#卷积神经网络的优缺点" class="headerlink" title="卷积神经网络的优缺点"></a>卷积神经网络的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>强大的特征学习能力：CNN能够自动从原始数据中学习和提取特征，这使得它在各种复杂任务中具有较高的性能。 </li><li>空间信息利用率高：CNN通过卷积操作能够充分利用输入数据的空间信息，从而在解决图像和自然语言处理问题时具有很好的效果。 </li><li>参数可学习性：CNN的参数可以通过反向传播算法进行学习和优化，这使得它能够对各种数据集进行有效的训练。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>计算量大：CNN需要进行大量的卷积计算，这需要大量的计算资源和时间。</li><li>参数众多：与其它深度学习模型相比，CNN的参数数量较多，这使得模型的训练和调优过程变得更加复杂。</li></ul><h2 id="先进的CNN模型"><a href="#先进的CNN模型" class="headerlink" title="先进的CNN模型"></a>先进的CNN模型</h2><p>近年来，许多研究者提出了许多先进的卷积神经网络模型，以下是其中几个比较流行并且应用广泛的模型：</p><ul><li><p>ResNet（残差网络）：ResNet通过引入“残差块”有效地解决了深度神经网络中的梯度消失问题，使得网络可以设计得更深，性能更好。ResNet的主要特点是跨层连接，它通过引入捷径连接技术（shortcut connections）将输入跨层传递并与卷积的结果相加。在ResNet中只有一个池化层，它连接在最后一个卷积层后面。ResNet使得底层的网络能够得到充分训练，准确率也随着深度的加深而得到显著提升。将深度为152层的ResNet用于LSVRC-15的图像分类比赛中，它获得了第1名的成绩。ResNet在多个计算机视觉任务中取得了优异的成绩，包括图像分类、目标检测和语义分割等。</p></li><li><p>DenseNet（稠密网络）：DenseNet通过在网络中引入了稠密连接，使得信息可以直接从输入层传播到输出层。这种连接方式减少了梯度消失的问题，并能够更好地传播特征，从而使得网络更加高效。DenseNet在多个任务中均取得了优秀的性能。</p></li><li><p>EfficientNet：EfficientNet是一种既具有高精度又具有低计算复杂度的轻量级网络。它通过改变网络深度、宽度和分辨率的方式来提高网络的性能和效率。EfficientNet在图像分类任务中表现优异，且计算效率高，适用于移动设备和嵌入式系统。</p></li><li><p>MobileNet：MobileNet是为了移动设备和嵌入式设备设计的轻量级网络。它通过使用深度可分离的卷积（depthwise separable convolution）来减少参数量和计算复杂性，同时保持较高的性能。MobileNet广泛应用于移动应用、嵌入式设备和物联网设备中。</p></li><li><p>Transformer：尽管Transformer最初应用于自然语言处理任务，但近年来已广泛应用于计算机视觉任务。在CNN的基础上，Transformer引入了自注意力机制（self-attention mechanism），使得模型能够更好地捕捉输入数据的局部和全局信息。基于Transformer的模型在图像分类、目标检测和语义分割等任务中均取得了重大突破。</p></li><li><p>Inception V3：Inception V3 是 2015 年提出的一种 CNN 模型，它通过 Inception 模块将不同大小的卷积核和池化层结合起来，提高了模型的性能。Inception V3 在图像识别领域取得了巨大的成功。</p></li><li><p>YOLOv4：YOLOv4 是 2020 年提出的一种实时目标检测模型，它通过改进特征提取、目标回归和非极大值抑制等技术，提高了模型的性能。YOLOv4 在目标检测领域取得了巨大的成功。</p></li></ul><p>这些模型都在深度学习领域有着广泛的应用，并且在各种计算机视觉任务中持续取得优异的成绩。虽然这些模型有许多优点，但它们也需要根据具体的应用场景和数据进行适当的调整和优化。此外，这些模型的实现也需要相应的硬件和软件支持，以确保其计算效率和训练效果。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文对CNN结构及原理进行概念性的描述，并未进行深入学术研究。 主要列举了当前应用比较广泛的CNN改进模型，以便大家在具体应用中对模型的选择进行参考。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247571595&idx=3&sn=c04072ee69a028264b797e76d9a42957&chksm=fb543a67cc23b37165943ca27442b0ab267106a9664399d8d856756339b4d226205da194c476&scene=27">https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247571595&amp;idx=3&amp;sn=c04072ee69a028264b797e76d9a42957&amp;chksm=fb543a67cc23b37165943ca27442b0ab267106a9664399d8d856756339b4d226205da194c476&amp;scene=27</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AI和机器学习综述</title>
      <link href="/2023/06/13/ai-ml/"/>
      <url>/2023/06/13/ai-ml/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么学习-AI"><a href="#为什么学习-AI" class="headerlink" title="为什么学习 AI"></a>为什么学习 AI</h2><p>随着 ChatGPT 的爆火, AI已成为新一轮科技革命的主要驱动力, 将贯穿各个领域, 各个行业. 比如金融 教育 军事 生命科学等.各大科技公司都在开发自己的大模型, 各国之间也展开了激烈的竞争. 可谓得 AI 者得天下.</p><p>机器学习模型需要结合各个垂直领域的数据及专业知识来进行优化, 训练, 调参, 以解决各种复杂的专业问题.  作为一个开发人员, 我们不仅要知道各种机器学习模型适合解决的问题, 还要学习其背后的算法原理, 才能更好的应用到我们自己需要解决的问题中.</p><p>每个人都应该学习如何使用它们来提高工作效率, 解决实际问题. 不一定每个人都要掌握背后复杂的机器学习算法及原理, 就像互联网刚兴起的时候, 我们只需要学习怎么使用电脑和网络, 并不需要掌握 Http 网络协议或者计算机底层架构. </p><p><strong>很喜欢的一句话: 种一棵树最好的时间是十年前, 其次就是现在 !</strong></p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>AGI: Artificial General Intelligence, “通用人工智能”，亦被称为强 AI，该术语指的是在任何你可以想象的人类的专业领域内，具备相当于人类智慧程度的 AI，一个 AGI 可以执行任何人类可以完成的任务。</li><li>AIGC: AI generated content，又称为生成式AI. AI从理解 识别内容，走向了自动生成内容，包括AIGC用于作画、图文、视频等多类型的内容创作.</li><li>LLM: Large Language Model, 大语言模型是指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。</li><li>Transformer: 是Google Brain 2017的提出的，它针对RNN的弱点进行重新设计，解决了RNN效率问题和传递中的缺陷等，在很多问题上都超过了RNN的表现.</li><li>GPT: Generative Pre-trained Transformer，是一种基于Transformer模型的预训练语言模型.</li><li>BERT: Bidirectional Encoder Representations from Transformers, 通过在大规模的无标注语料上进行预训练学习，得到的模型可以应用于多个类型的下游任务，具体实现方式就是在具体下游任务的数据集上进行微调（fine-turn）学习。预训练的过程可以理解为我们学习基础知识，比如学习英语时的背单词，而微调就是具体的学习任务，比如完形填空，阅读理解等具体任务。</li><li>ML: Machine Learning, 是实现人工智能的一种最主要的技术</li><li>DL: Deep Learning, 是机器学习中的一个分支</li></ul><table><thead><tr><th>LLM</th><th>Comp.</th><th>参数量(十亿)</th><th>token量(万亿)</th><th>应用</th></tr></thead><tbody><tr><td>GPT</td><td>OpenAI</td><td>175B</td><td>-</td><td>ChatGPT</td></tr><tr><td>Palm2</td><td>Google</td><td>54B</td><td>3.6</td><td>Bard</td></tr><tr><td>LLaMA</td><td>Meta</td><td>-</td><td>1.4</td><td>Jarvis</td></tr></tbody></table><h2 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h2><img src="/2023/06/13/ai-ml/ml-01.png" class=""><ul><li><p>强化学习: 高阶的ML, 通过大量的action和reward 训练出policy使reward最大化</p></li><li><p>传统机器学习: 应用于早期比较简单数据量比较少的任务, 需要手动提取特征</p><ul><li>SVM 支持向量机</li><li>逻辑回归</li><li>线性回归</li><li>决策树</li><li>随机森林</li></ul></li><li><p>深度学习: 复杂数据量大的任务 CV, NLP, AR</p><ul><li>ANN: 人工神经网络 简称 NN (Neural Network) </li><li>RNN: 循环神经网络 Recurrent NN </li><li>CNN: 卷积神经网络 Convolutional NN</li><li>LSTM: 长短期记忆</li></ul></li></ul><h2 id="常用的机器学习库"><a href="#常用的机器学习库" class="headerlink" title="常用的机器学习库"></a>常用的机器学习库</h2><ul><li>TensorFlow: Google开源API 兼容Python 和 C, 前身是google神经网络算法库 “DistBelief”</li><li>PyTorch: 是一个开源的Python机器学习库，基于Torch，用于自然语言处理等应用程序。PyTorch既可以看作加入了GPU支持的numpy，同时也可以看成一个拥有自动求导功能的强大的深度神经网络.</li><li>scikit-learn(SKLearn) , 包含了除了深度学习和强化学习的几乎所有传统模型. Python的免费机器学习库. 具有各种分类,回归,聚类算法: 支持向量机 随机森林 梯度提升 … 它底层使用Scipy数据结构，与Python中其余使用Scipy、Numpy、Pandas和Matplotlib进行科学计算的部分适应地很好.</li><li>Keras：Python的人工神经网络开源库 支持后台转为TensorFlow &#x2F; Microsoft-CNTK &#x2F; Theano</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深入理解贝叶斯公式：原理与应用</title>
      <link href="/2023/05/18/bayes/"/>
      <url>/2023/05/18/bayes/</url>
      
        <content type="html"><![CDATA[<blockquote><p>“人生中最重要的问题，在绝大多数情况下，真的就只是概率问题。”— 皮埃尔-西蒙·拉普拉斯</p></blockquote><p>贝叶斯公式把数学公式上升到了哲学的高度，进化到了认知世界的方法论上。 用白话来阐述就是我们最新的认知，来源于我们过往的知识及所看到的事实。最近正在读丹尼尔·卡尼曼的《思考，快与慢》，其中的核心思想是人们在做决策时，往往会受到快系统（直觉思维）的影响而产生一些偏见，这些偏见有时会导致人们的决策行为偏离基本的概率论原理，同时强调了初始概率对贝叶斯方法的重要性。<br>在现实生活中有很多事情我们没有办法做出100%正确的判断，只能通过掌握的知识和数据去逼近真相从而做出尽可能正确的选择。比如在投资中最重要的事情就是管理概率， 只要保证我们买入的资产未来大概率会上涨， 就能在长期投资中稳定盈利。而管理概率最重要的工具就是贝叶斯定理。</p><p>贝叶斯公式是概率论中的重要工具，也是机器学习领域中常用的基础算法之一。它主要用于描述在给定条件下，某一事件发生的概率。在分类、预测等众多任务中，贝叶斯公式可以根据已有的数据集，快速有效地进行分类和预测。本篇文章将详细介绍贝叶斯公式的概念、原理和应用，帮助读者更好地理解和应用这一重要公式。</p><h2 id="贝叶斯公式的基本概念和原理"><a href="#贝叶斯公式的基本概念和原理" class="headerlink" title="贝叶斯公式的基本概念和原理"></a>贝叶斯公式的基本概念和原理</h2><ul><li>条件概率：在事件B已经发生的情况下，事件A发生的概率称为条件概率，记作P(A|B)。</li><li>联合概率：两个事件A和B同时发生的概率称为联合概率，记作P(AB)。</li></ul><p>全概公式: </p><p><code>P(B) = P(A1)*P(B|A1) + ... + P(An)*P(B|An)</code></p><p>贝叶斯公式： </p><p><code>P(Ai|B) = P(AiB) / P(B) = P(Ai) * P(B|Ai) / P(A1)*P(B|A1) + ... + P(An)*P(B|An)</code></p><p>它表示在给定事件B发生的情况下，事件A发生的概率等于事件A和事件B同时发生的概率除以事件B发生的概率。</p><h2 id="贝叶斯公式的示例"><a href="#贝叶斯公式的示例" class="headerlink" title="贝叶斯公式的示例"></a>贝叶斯公式的示例</h2><p>假设有一个袋子D1和D2，其中有红球和蓝球，已知袋D1中红球的比例为0.6，D2中红球的比例为0.4， 现在从袋中随机取出一个球，发现这个球是红球。那么这个球来自D1的概率是多少？</p><p>根据贝叶斯公式，可以得到:</p><pre class="line-numbers language-none"><code class="language-none">P(D1｜红球) &#x3D; P(红球｜D1) * P(D1) &#x2F; P(红球)&#x3D; 0.6 * 0.5 &#x2F; (0.6 * 0.5 + 0.4*0.5) &#x3D; 0.3&#x2F;0.5&#x3D; 0.6<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="贝叶斯公式在机器学习中的应用"><a href="#贝叶斯公式在机器学习中的应用" class="headerlink" title="贝叶斯公式在机器学习中的应用"></a>贝叶斯公式在机器学习中的应用</h2><p>贝叶斯公式在机器学习中有着广泛的应用，其中最著名的就是朴素贝叶斯分类器。朴素贝叶斯分类器是一种基于贝叶斯定理与特征之间独立假设的分类方法，主要用于文本分类和垃圾邮件过滤等任务。</p><ul><li>文本分类：文本分类是机器学习的一个重要应用，朴素贝叶斯分类器利用文本中的词频统计信息，将每个文本表示为一个词频向量，并根据贝叶斯公式计算文本属于不同类别的概率，最终将文本划分到概率最高的类别中。</li><li>垃圾邮件过滤：垃圾邮件过滤是另一个常见的应用场景，朴素贝叶斯分类器可以通过学习正常邮件和垃圾邮件的特征，利用贝叶斯公式计算新邮件属于不同类别的概率，并将新邮件分类到相应的类别中。</li></ul><p>除了朴素贝叶斯分类器外，贝叶斯网络也是机器学习中常用的方法之一。贝叶斯网络是一种概率图模型，用于描述变量之间的依赖关系和条件独立关系。通过学习和推理贝叶斯网络，可以解决诸如分类、异常检测、推荐系统等任务。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>贝叶斯公式具有广泛的应用场景和坚实的理论基础，在机器学习和概率论领域中扮演着重要的角色。<br>正如《底层逻辑》一书中所说，数学中的三大逻辑—数学期望、大数定律和条件概率，以及条件概率推演出来的“神器”贝叶斯定理，能有效地指导创业与投资，使你走向成功。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Amazon SageMaker</title>
      <link href="/2023/05/17/amazon-sagemaker/"/>
      <url>/2023/05/17/amazon-sagemaker/</url>
      
        <content type="html"><![CDATA[<p>这篇文章旨在给大家推荐一个很好用的机器学习平台，或者说机器学习的集成开发环境：Amazon SageMaker。单纯就是因为它真的很方便实用，它集成了数据预处理接口以及常用的机器学习库（PyTorch、TensorFlow、MXNet、Chainer、Gluon、Keras、Scikit-learn等）和预训练的开源大模型（ Llama-2， Falcon， Stable Diffusion等），可供我们很方便的调用，大大缩短了开发时间。训练好的模型还可以直接部署到AWS上。<br>我会在后面的文章中给出用Sagemaker训练部署机器学习模型的例子。</p><h2 id="Amazon-SageMaker-简介"><a href="#Amazon-SageMaker-简介" class="headerlink" title="Amazon SageMaker 简介"></a>Amazon SageMaker 简介</h2><p>Amazon SageMaker 是一项完全托管的机器学习服务。借助 SageMaker 开发人员可以快速、轻松地构建和训练机器学习模型，然后直接将模型部署到生产环境中。它提供了一个集成的 Jupyter Notebook 实例，可以轻松访问各种数据源进行分析处理，无需我们自己去管理服务器。<br>SageMaker提供了常见的机器学习算法，并且这些算法经过了优化，可以在分布式环境中高效处理大数据。</p><h2 id="Sagemaker-集成开发环境"><a href="#Sagemaker-集成开发环境" class="headerlink" title="Sagemaker 集成开发环境"></a>Sagemaker 集成开发环境</h2><p>SageMaker有几个集成开发环境：Sagemaker Studio，Sagemaker Studio Lab, Sagemaker RStudio, SageMaker Canvas.</p><p>如果你是一个机器学习的小白，你可以用 SageMaker Autopilot。只需要提供原始数据和任务描述，就可以自动完成所有task，包括根据任务类型自动选取合适的机器学习算法。</p><p>最常用的就是Sagemaker Studio， 下面介绍其使用过程。</p><h2 id="Sagemaker-Studio-的使用过程"><a href="#Sagemaker-Studio-的使用过程" class="headerlink" title="Sagemaker Studio 的使用过程"></a>Sagemaker Studio 的使用过程</h2><p>Sagemaker Studio就是一个网页版的机器学习的IDE， 可以很方便的 build, train, debug, deploy, and monitor你的机器学习模型。<br>使用之前需要注册一个Amaozn AWS账号，然后申请一个Domain： <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html</a></p><p>从零开始完成一个机器学习应用的任务主要分为4个步骤： 数据预处理，模型构建，训练和微调，模型部署和管理，监测。SageMaker 提供一个notebook instance来跑Jupyter Notebook。上面这些步骤都是跑在notebook里的python代码， 修改调试都很方便。</p><ol><li>数据预处理<br>在训练ML模型之前，我们通常需要对采集的数据进行分析和处理</li></ol><ul><li><p>Data Wrangler : 是专门为ML准备的数据预处理模块，内置了300多个数据转换接口，让我们很方便的集成和处理（选取，清洗，正则化，归一化，缺失值处理等）原始数据然后接入到我们的ML模型去训练。参考： <a href="https://aws.amazon.com/sagemaker/data-wrangler/">https://aws.amazon.com/sagemaker/data-wrangler/</a></p><p>  <img src="/./Amazon-SageMaker/sage-1.png"></p></li></ul><ol start="2"><li><p>训练测试调参<br>要训练ML模型你要构建自己的机器学习算法或者使用已有的基础模型。SageMaker提供内置的机器学习算法和预训练的模型， 我们需要根据自己要解决的问题去选择一个合适的算法， 这一点很关键，选择往往大于努力。</p><p> <img src="/./Amazon-SageMaker/sagemaker-2.png"></p><p> 完成模型的训练以后，需要用测试数据集对模型的预测结果进行评价，看预测结果的精度是否符合预期。如果不符合， 需要去调参。</p></li><li><p>部署模型<br>当你完成模型训练和微调之后，你可以把模型的artifact存放到S3，然后用SageMaker Inference进行部署。</p><p> <img src="/./Amazon-SageMaker/sage-3.png"></p><ul><li>在SageMaker Inference设置好你训练完的model 及 生成的Docker image</li><li>设置inference选项： Real-Time &#x2F; Asynchronous &#x2F; Serverless &#x2F; Batch Transform，这里需要根据自己产品的实际需求去选择，比如你是一个股票价格预测模型，需要根据分时数据进行预测并做决策，那就要设置成Real-Time</li><li>根据实际需求选择运行model的实例类型及个数</li><li>创建 Inference endpoint实例</li><li>激活实例并接收预测结果数据</li></ul></li><li><p>监测<br>模型部署完以后，就会自动生成一个metrics dashboard， 我们可以监测模型的健康状况，比如 Invocation error，lantency，CPU&#x2F;Memery usage等<br>同时我们可以监测模型的预测质量，比如准确率，漂移等。 还可以和实际（ground truth）数据进行对比。并且用最新收集的实际数据继续训练模型，形成一个自动优化的闭环。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络-RNN（Recurrent Neural Network）</title>
      <link href="/2023/05/15/rnn/"/>
      <url>/2023/05/15/rnn/</url>
      
        <content type="html"><![CDATA[<p>RNN是一种特殊的神经网络结构, 它是根据”人的认知是基于过往的经验和记忆”这一观点提出的. 它与DNN,CNN不同的是: 它不仅考虑当前时刻的输入,而且赋予了网络对前面的内容的一种’记忆’功能.</p><p>RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p><h2 id="RNN的应用领域"><a href="#RNN的应用领域" class="headerlink" title="RNN的应用领域:"></a>RNN的应用领域:</h2><p>只要考虑时间先后顺序的问题都可以使用RNN来解决.这里主要说一下几个常见的应用领域</p><ul><li><p>自然语言处理(NLP): 主要有视频处理, 文本生成, 语言模型, 图像处理</p></li><li><p>机器翻译, 机器写文章</p></li><li><p>语音识别</p></li><li><p>图像描述生成</p></li><li><p>文本相似度计算</p></li><li><p>推荐系统。例如：音乐推荐、网易考拉商品推荐、Youtube视频推荐等新的应用领域。</p></li></ul><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>如下图所示, 我们可以看到RNN层级结构较之于CNN来说比较简单, 它主要有输入层,Hidden Layer, 输出层组成.</p><p>并且会发现在Hidden Layer 有一个箭头表示数据的循环更新, 这个就是实现时间记忆功能的方法.</p><img src="/2023/05/15/rnn/RNN-1.png" class=""><p>t-1, t, t+1表示时间序列. X表示输入的样本. St表示样本在时间t处的的记忆 <code>St = f(W*St-1 +U*Xt)</code> W表示输入的权重, U表示此刻输入的样本的权重, V表示输出的样本权重. 在t &#x3D;1时刻, 一般初始化输入S0&#x3D;0, 随机初始化W,U,V, 进行下面的公式计算:</p><img src="/2023/05/15/rnn/RNN-2.png" class=""><p>其中,f和g均为激活函数. 其中f可以是tanh,relu,sigmoid等激活函数，g通常是softmax也可以是其他。 时间就向前推进，此时的状态s1作为时刻1的记忆状态将参与下一个时刻的预测活动，也就是:</p><img src="/2023/05/15/rnn/RNN-3.png" class=""><p>以此类推, 可以得到最终的输出值为:</p><img src="/2023/05/15/rnn/RNN-4.png" class=""><h1 id="RNN-的缺陷及改进算法"><a href="#RNN-的缺陷及改进算法" class="headerlink" title="RNN 的缺陷及改进算法"></a>RNN 的缺陷及改进算法</h1><p>RNN 处理时间序列的问题的效果很好, 但是仍然存在着一些问题, 其中较为严重的是容易出现梯度消失或者梯度爆炸的问题, 梯度消失主要指由于时间过长而造成记忆值较小的现象.</p><p>为了缓解传递间的梯度和遗忘问题，设计了各种各样的 RNN cell，最著名的两个就是LSTM和GRU.</p><p>对于梯度消失: 由于它们都有特殊的方式存储”记忆”，那么以前梯度比较大的”记忆”不会像简单的RNN一样马上被抹除，因此可以一定程度上克服梯度消失问题。</p><p>对于梯度爆炸: gradient clipping，就是当你计算的梯度超过阈值c或者小于阈值-c的时候，便把此时的梯度设置成c或-c。 </p><p>LSTM (Long Short Term Memory): </p><img src="/2023/05/15/rnn/LSTM.png" class=""><p>GRU (Gated Recurrent Unit) : </p><img src="/2023/05/15/rnn/GRU.png" class=""><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>ransformer 是Google Brain 2017的提出的一篇论文，它针对RNN的弱点进行重新设计，解决了RNN效率问题和传递中的缺陷等，在很多问题上都超过了RNN的表现。Transfromer的基本结构如下图所示，它是一个N进N出的结构，也就是说每个Transformer单元相当于一层的RNN层，接收一整个句子所有词作为输入，然后为句子中的每个词都做出一个输出。但是与RNN不同的是，Transformer能够同时处理句子中的所有词，并且任意两个词之间的操作距离都是1，这么一来就很好地解决了上面提到的RNN的效率问题和距离问题。</p><img src="/2023/05/15/rnn/transformer.png" class=""><p>参考 :</p><ol><li><a href="https://zhuanlan.zhihu.com/p/69290203">https://zhuanlan.zhihu.com/p/69290203</a></li><li><a href="https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/118029517">https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/118029517</a></li><li><a href="https://blog.csdn.net/qq_39521554/article/details/80083592">https://blog.csdn.net/qq_39521554/article/details/80083592</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
